output_filename: "/Users/orpheus/Documents/Masters/NeuralQuantumState/data/fermion_dots/"
nparticles: 2 # 6, 10, 12, 20
dim: 2 # do not change this in this yaml
nsamples: 2097152 # 2^12
nchains: 8
eta: 0.01 #0.034052578292785
training_cycles: 3000
mcmc_alg: "m"
optimizer: "rmsprop"
batch_size: 500
detailed: true
nqs_type: "vmc"
backend: "jax"
seed: 421
save_positions: true
particle: "fermion_dots"
correlation: "pj"
interaction_type: "coulomb_gradual" # "coulomb" or "none" or "coulomb_gradual" 
r0_reg: 3 # larger makes change faster. Smaller is more gradual. Range should be from 1 to 5 kinda. This will only make sense if coulomb_gradual is chosen.
latent_dim: 17
omega: 1.0 # 1/6, 0.28, 0.5, 1.0
pretrain: false
base_layer_sizes:
  dsffn:
    S0: [14, 9, 7, 5, 3]
    S1: [7, 5, 3, 1] # first node is latent dim, last node always 1
activations:
  dsffn: 
    S0: ["gelu", "elu", "gelu", "elu", "gelu", "elu"]
    S1: ["gelu", "elu", "gelu", "linear"]
nhidden: 6
architectures:
  arch1:
    activations:
      dsffn:
        S0:
          - gelu
          - gelu
          - gelu
          - gelu
        S1:
          - gelu
          - linear
    base_layer_sizes:
      dsffn:
        S0:
          - 7
          - 5
          - 3
        S1:
          - 3
          - 1
  arch2:
    activations:
      dsffn:
        S0:
          - gelu
          - gelu
          - gelu
          - gelu
          - gelu
        S1:
          - gelu
          - gelu
          - gelu
          - linear
    base_layer_sizes:
      dsffn:
        S0:
          - 10
          - 7
          - 5
          - 3
        S1:
          - 6
          - 4
          - 2
          - 1
  arch3:
    activations:
      dsffn:
        S0:
          - gelu
          - gelu
          - gelu
          - gelu
        S1:
          - gelu
          - gelu
          - gelu
          - linear
    base_layer_sizes:
      dsffn:
        S0:
          - 8
          - 6
          - 4
        S1:
          - 4
          - 2
          - 1
  arch4:
    activations:
      dsffn:
        S0:
          - gelu
          - gelu
          - gelu
          - gelu
          - gelu
        S1:
          - gelu
          - gelu
          - linear
    base_layer_sizes:
      dsffn:
        S0:
          - 9
          - 7
          - 5
          - 3
        S1:
          - 5
          - 3
          - 1
  arch5:
    activations:
      dsffn:
        S0:
          - gelu
          - gelu
          - gelu
          - gelu
          - gelu
        S1:
          - gelu
          - gelu
          - linear
    base_layer_sizes:
      dsffn:
        S0:
          - 14
          - 9
          - 7
          - 5
        S1:
          - 5
          - 3
          - 1
  arch6:
    activations:
      dsffn:
        S0:
          - gelu
          - gelu
          - gelu
        S1:
          - gelu
          - gelu
          - linear
    base_layer_sizes:
      dsffn:
        S0:
          - 6
          - 4
        S1:
          - 3
          - 2
          - 1
  arch7:
    activations:
      dsffn:
        S0:
          - gelu
          - gelu
          - gelu
          - gelu
          - gelu
        S1:
          - gelu
          - gelu
          - linear
    base_layer_sizes:
      dsffn:
        S0:
          - 12
          - 9
          - 7
          - 5
        S1:
          - 6
          - 4
          - 1
  arch8:
    activations:
      dsffn:
        S0:
          - gelu
          - gelu
          - gelu
        S1:
          - gelu
          - linear
    base_layer_sizes:
      dsffn:
        S0:
          - 9
          - 6
        S1:
          - 3
          - 1