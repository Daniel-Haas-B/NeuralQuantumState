{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add relative path to sys.path\n",
    "\n",
    "sys.path.append(\"/Users/haas/Documents/Masters/GANQS/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffnn import FFNN\n",
    "from numpy.random import default_rng\n",
    "from jax import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "nparticles = 3\n",
    "dim = 2\n",
    "layer_sizes = [3, 3, 1]\n",
    "activations = [\"sigmoid\", \"sigmoid\", \"exp\"]\n",
    "factor = 1.0\n",
    "sigma2 = 1.0\n",
    "\n",
    "# Create a random set of positions (replace this with your actual data)\n",
    "rng = np.random.default_rng()\n",
    "r_test = rng.standard_normal(size=nparticles * dim)\n",
    "\n",
    "# Create an instance of FFNN\n",
    "ffnn = FFNN(nparticles, dim, layer_sizes, activations, factor, sigma2, rng=rng)\n",
    "grad_wf = ffnn.grad_wf(r_test) # grad wrt position\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def finite_difference_gradient(func, x, epsilon=1e-4):\n",
    "    grad_approx = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_plus = np.copy(x)\n",
    "        x_minus = np.copy(x)\n",
    "        x_plus[i] += epsilon\n",
    "        x_minus[i] -= epsilon\n",
    "        grad_approx[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n",
    "    return grad_approx\n",
    "\n",
    "grad_wf_approx = finite_difference_gradient(lambda r: ffnn.wf(r, ffnn.params), r_test)\n",
    "\n",
    "print(\"diff between grad_wf and grad_wf_approx:\", np.linalg.norm(grad_wf - grad_wf_approx))\n",
    "\n",
    "\n",
    "#Now wrt parameters\n",
    "import copy\n",
    "def finite_difference_param_gradient(func, params, epsilon=1e-7):\n",
    "    grad_approx = {}\n",
    "\n",
    "    for key in params.keys():\n",
    "\n",
    "        param_shape = params.get(key).shape\n",
    "        grad_approx[key] = np.zeros(param_shape)\n",
    "\n",
    "        for index in np.ndindex(param_shape):\n",
    "            params_plus = copy.deepcopy(params)\n",
    "            # convert to np array\n",
    "            params_plus.set(key, np.array(params_plus.get(key)))\n",
    "            params_minus = copy.deepcopy(params)\n",
    "            # convert to np array\n",
    "            params_minus.set(key, np.array(params_minus.get(key)))\n",
    "\n",
    "            params_plus.get(key)[index] += epsilon\n",
    "            params_minus.get(key)[index] -= epsilon\n",
    "\n",
    "            grad_approx[key][index] = (func(params_plus) - func(params_minus)) / (2 * epsilon)\n",
    "\n",
    "    return grad_approx\n",
    "\n",
    "# Test the parameter gradient function\n",
    "func_to_test = lambda p: ffnn.wf(r_test, p)  # The function to compute the wave function\n",
    "grad_params_computed = ffnn.grads(r_test)  # Computed gradients using your implementation\n",
    "grad_params_approx = finite_difference_param_gradient(func_to_test, ffnn.params)\n",
    "\n",
    "# Compare the results\n",
    "for key in grad_params_computed.keys():\n",
    "    print(f\"Parameter: {key}\")\n",
    "\n",
    "    print(\"diff between grad_params and grad_params_approx:\", np.linalg.norm(grad_params_computed[key] - grad_params_approx[key]))\n",
    "    \n",
    "\n",
    "def finite_difference_laplacian(func, r, epsilon=1e-4):\n",
    "    laplacian_approx = np.zeros_like(r)\n",
    "\n",
    "    for i in range(len(r)):\n",
    "        r_plus_epsilon = np.copy(r)\n",
    "        r_minus_epsilon = np.copy(r)\n",
    "        r_plus_2epsilon = np.copy(r)\n",
    "        r_minus_2epsilon = np.copy(r)\n",
    "\n",
    "        r_plus_epsilon[i] += epsilon\n",
    "        r_minus_epsilon[i] -= epsilon\n",
    "        r_plus_2epsilon[i] += 2 * epsilon\n",
    "        r_minus_2epsilon[i] -= 2 * epsilon\n",
    "\n",
    "        # Approximate second derivative for each dimension\n",
    "        laplacian_approx[i] = (-func(r_plus_2epsilon) + 16*func(r_plus_epsilon) - 30*func(r) + 16*func(r_minus_epsilon) - func(r_minus_2epsilon)) / (12 * epsilon**2)\n",
    "\n",
    "    # Sum over all dimensions to get the total Laplacian\n",
    "    return laplacian_approx.sum()\n",
    "\n",
    "laplacian_computed = ffnn.laplacian(r_test)  # Your Laplacian computation\n",
    "laplacian_approx = finite_difference_laplacian(lambda r: ffnn.wf(r, ffnn.params), r_test)\n",
    "\n",
    "# Compare the results\n",
    "print(\"Computed Laplacian:\", laplacian_computed)\n",
    "print(\"Approximated Laplacian:\", laplacian_approx)\n",
    "print(\"Difference:\", np.abs(laplacian_computed - laplacian_approx))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "540899a6d5f72233f45842563b6aafa756d796888aa192a1b7fe41908bbe83fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
