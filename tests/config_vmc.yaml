nparticles: 2
dim: 2
nsamples: 16384  # 2**14
nchains: 1
eta: 0.0003535533905932738  # 0.001 / np.sqrt(nparticles * dim)

training_cycles: 10
mcmc_alg: "m"
optimizer: "sr"
batch_size: 100  # initial batch size
detailed: true
seed: 42
wf_type: "vmc"

common_kwargs:
    correlation: None  # or just j or None (default)
    symmetry: "fermion" # why does this change the pretrain? and should it?
